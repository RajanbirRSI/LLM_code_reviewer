name: Automated Code Review (Bash Version)

on:
  workflow_dispatch:
  push:
    branches-ignore:
      - main
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  code-review:
    runs-on: self-hosted
    
    # Set default shell to bash for all steps
    defaults:
      run:
        shell: bash
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check powershell is working
        shell: pwsh
        run: |
         pwsh -Command "Write-Host 'PowerShell Core is working!'"
         
      - name: Check Python
        shell: pwsh  # Use Bash
        run: |
          python --version
          pip --version

      - name: Install dependencies
        shell: pwsh
        run: |
          if (Test-Path "requirements.txt") {
            pip install -r requirements.txt
          }
          pip install requests PyGithub

      - name: Check Ollama service
        shell: pwsh
        run: |
          try {
            ollama ps
            Write-Host "Ollama is running"
          } catch {
            Write-Host "Starting Ollama service..."
            Start-Process "ollama" -ArgumentList "serve" -WindowStyle Hidden
            Start-Sleep 10
            ollama ps
          }

      - name: Ensure required models are available
        shell: pwsh
        run: |
          $models = ollama list
          if ($models -notmatch "mistral") {
            Write-Host "Pulling Mistral model..."
            ollama pull mistral
          }

      - name: Run automated code reviewer
        id: code-review
        shell: bash  # Your original bash syntax works here
        run: |
          echo "=== Running Code Review ==="
          
          # Run the Python script with explicit error handling
          set +e  # Don't exit on error immediately
          python automated_code_reviewer.py > review_output.txt 2>&1
          python_exit_code=$?
          set -e  # Re-enable exit on error
          
          echo "Python script exit code: $python_exit_code"
          
          # Always show what was captured
          if [ -f "review_output.txt" ]; then
            echo "=== Review Output Content ==="
            cat review_output.txt
            echo "=== End of Review Output ==="
          else
            echo "‚ùå Review output file was not created!"
            exit 1
          fi
          
          # Check if Python script failed
          if [ $python_exit_code -ne 0 ]; then
            echo "‚ùå Python script failed with exit code: $python_exit_code"
            # Don't exit here, try to extract score anyway
          fi
          
          # Extract score using compatible grep (without -P flag)
          # Try multiple patterns to match different output formats
          SCORE=""
          
          # Pattern 1: "Overall Score: 85"
          SCORE=$(grep -i "overall score" review_output.txt | grep -o '[0-9]\+' | head -1 || echo "")
          
          # Pattern 2: "Score: 85" 
          if [ -z "$SCORE" ]; then
            SCORE=$(grep -i "^score" review_output.txt | grep -o '[0-9]\+' | head -1 || echo "")
          fi
          
          # Pattern 3: Any line ending with "/100"
          if [ -z "$SCORE" ]; then
            SCORE=$(grep -o '[0-9]\+/100' review_output.txt | grep -o '^[0-9]\+' | head -1 || echo "")
          fi
          
          # Pattern 4: Any number between 0-100 (last resort)
          if [ -z "$SCORE" ]; then
            SCORE=$(grep -o '\b[0-9]\{1,3\}\b' review_output.txt | head -1 || echo "")
            # Validate it's a reasonable score (0-100)
            if [ ! -z "$SCORE" ] && ([ "$SCORE" -lt 0 ] || [ "$SCORE" -gt 100 ]); then
              SCORE=""
            fi
          fi
          
          # Default score if nothing found
          if [ -z "$SCORE" ]; then
            echo "‚ö†Ô∏è Could not extract score from output, defaulting to 0"
            SCORE=0
          else
            echo "‚úÖ Extracted Score: $SCORE"
          fi
          
          # Set outputs for next steps
          echo "score=$SCORE" >> $GITHUB_OUTPUT
          
          # Determine pass/fail
          if [ "$SCORE" -ge 75 ]; then
            echo "passed=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Review Passed: YES (Score: $SCORE/100)"
          else
            echo "passed=false" >> $GITHUB_OUTPUT
            echo "‚ùå Review Failed: NO (Score: $SCORE/100)"
          fi

      - name: Debug outputs
        shell: bash
        run: |
          echo "=== Debug Outputs ==="
          echo "Score output: ${{ steps.code-review.outputs.score }}"
          echo "Passed output: ${{ steps.code-review.outputs.passed }}"
      # Rest of the steps remain the same as the PowerShell version
      - name: Comment on PR with review results
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let reviewContent = '';
            try {
              reviewContent = fs.readFileSync('review_output.txt', 'utf8');
            } catch (error) {
              reviewContent = 'Error reading review output';
            }
            
            const score = '${{ steps.code-review.outputs.score }}';
            const passed = '${{ steps.code-review.outputs.passed }}' === 'true';
            
            const commentBody = `
            ## ü§ñ Automated Code Review Results
            
            **Overall Score: ${score}/100**
            **Status: ${passed ? '‚úÖ PASSED' : '‚ùå FAILED'}**
            
            <details>
            <summary>üìã Detailed Review</summary>
            
            \`\`\`
            ${reviewContent}
            \`\`\`
            
            </details>
            
            ${passed ? 
              'üéâ Great job! Your code meets the quality standards.' : 
              '‚ö†Ô∏è Please address the issues above before merging.'
            }
            
            ---
            *Generated by LLM Code Reviewer using Mistral on Windows self-hosted runner*
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: commentBody
            });

      - name: Upload review artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: code-review-results
          path: |
            review_output.txt
            *.log
          retention-days: 30
